import base64
import logging
from typing import List

from langchain_core.documents import Document
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables.base import Runnable
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import ChatGoogleGenerativeAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

template = """Based on the provided context, answer the question at the end.
If the answer is unknown, state that you do not know; do not fabricate a response.
Provide a helpful and accurate answer, referencing the context wherever applicable.

{context}

Question: {question}

Answer:"""

prompt = PromptTemplate.from_template(template)
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0)


def _save_doc_locally(pdf_string: str, path: str = "./document.pdf") -> str:
    logger.info("Saving PDF document to local storage")
    decoded_bytes = base64.b64decode(pdf_string)
    if decoded_bytes[0:4] != b"%PDF":
        raise TypeError("Invalid PDF file received")

    with open(path, "wb") as _f:
        _f.write(decoded_bytes)

    return path


def _load_and_split_doc(pdf_path: str) -> List[Document]:
    logger.info("Splitting PDF document into text chunks for embedding")
    loader = PyPDFLoader(pdf_path)
    splits = loader.load_and_split(
        text_splitter=RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50, add_start_index=True
        )
    )
    return splits


def _get_embedding_retriever(splits: List[Document]) -> VectorStoreRetriever:
    logger.info("Embedding document chunks into vector space")
    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_function)
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 5}
    )
    return retriever


def _format_docs(docs: List[str]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


def _get_chain(
        retriever: VectorStoreRetriever, prompt: PromptTemplate, llm: BaseChatModel
) -> Runnable:
    logger.info("Constructing RAG chain")
    rag_chain = (
            {"context": retriever | _format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )
    return rag_chain


def get_answer(pdf_string: str, question: str) -> str:
    logger.info("Starting RAG chain")
    pdf_path = _save_doc_locally(pdf_string, path="./document.pdf")
    splits = _load_and_split_doc(pdf_path)
    retriever = _get_embedding_retriever(splits)
    rag_chain = _get_chain(retriever, prompt, llm)

    logger.info("Invoking RAG chain")
    response = rag_chain.invoke(question)

    logger.info("Returning response generated by RAG chain")
    return response
